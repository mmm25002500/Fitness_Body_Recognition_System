#!/usr/bin/env python3
"""
æ¸¬è©¦ BiLSTM æ¨¡å‹çš„é æ¸¬åŠŸèƒ½
ä½¿ç”¨æ–¹å¼: python test_model.py <video_path>
"""
import torch
import cv2
import numpy as np
import sys
from pathlib import Path

from pose_extractor_mediapipe import PoseExtractorMediaPipe
from model import BiLSTMAttention
from feature_utils_v2 import landmarks_to_features_v2

# é‹å‹•é¡åˆ¥
exercise_names = [
    "Barbell Biceps Curl",
    "Hammer Curl",
    "Push-up",
    "Shoulder Press",
    "Squat"
]

def test_video(video_path, window_size=45, stride=3):
    """æ¸¬è©¦å–®ä¸€å½±ç‰‡ - ä½¿ç”¨æ»‘å‹•çª—å£é æ¸¬"""
    print(f"ğŸ§ª æ¸¬è©¦å½±ç‰‡: {video_path}")
    print("=" * 60)

    # è¼‰å…¥æ¨¡å‹
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ“± ä½¿ç”¨è¨­å‚™: {device}")

    model = BiLSTMAttention(
        input_dim=102,
        hidden_dim=96,
        attn_dim=128,
        num_classes=5
    )
    checkpoint = torch.load("bilstm_mix_best_pt.pth", map_location=device, weights_only=True)
    model.load_state_dict(checkpoint)
    model.to(device)
    model.eval()
    print("âœ“ æ¨¡å‹è¼‰å…¥æˆåŠŸ (BiLSTMAttention, 102 dims)")

    # åˆå§‹åŒ–å§¿æ…‹æå–å™¨
    pose_extractor = PoseExtractorMediaPipe()
    print("âœ“ MediaPipe åˆå§‹åŒ–å®Œæˆ")

    # è®€å–å½±ç‰‡
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"âŒ ç„¡æ³•é–‹å•Ÿå½±ç‰‡: {video_path}")
        return None

    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    print(f"ğŸ“¹ å½±ç‰‡è³‡è¨Š: {frame_count} å¹€, FPS={fps:.1f}")
    print()

    # æå–ç‰¹å¾µ
    frame_features = []
    frame_idx = 0

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        landmarks, _ = pose_extractor.extract_landmarks(frame)
        if landmarks is not None:
            features = landmarks_to_features_v2([landmarks])[0]
            frame_features.append(features)

        frame_idx += 1
        if frame_idx % 100 == 0:
            print(f"è™•ç†ä¸­... {frame_idx}/{frame_count} å¹€")

    cap.release()

    print(f"\nâœ“ æˆåŠŸæå– {len(frame_features)} å¹€ç‰¹å¾µ")
    print(f"âœ“ ç‰¹å¾µç¶­åº¦: {len(frame_features[0]) if frame_features else 0}")

    if len(frame_features) < window_size:
        print(f"âŒ å½±ç‰‡å¤ªçŸ­ï¼è‡³å°‘éœ€è¦ {window_size} å¹€ï¼Œä½†åªæœ‰ {len(frame_features)} å¹€")
        return None

    # æ»‘å‹•çª—å£é æ¸¬
    predictions = []
    prediction_details = []

    print(f"\nğŸ”„ ä½¿ç”¨æ»‘å‹•çª—å£é æ¸¬ (window_size={window_size}, stride={stride})...")

    for i in range(0, len(frame_features) - window_size + 1, stride):
        window = frame_features[i:i + window_size]
        window_tensor = torch.FloatTensor([window]).to(device)

        with torch.no_grad():
            output = model(window_tensor)
            probabilities = torch.softmax(output, dim=1)
            pred_class = torch.argmax(probabilities, dim=1).item()
            confidence = probabilities[0][pred_class].item()

            predictions.append(pred_class)
            prediction_details.append({
                "window": i,
                "class": pred_class,
                "confidence": confidence,
                "probabilities": probabilities[0].cpu().numpy()
            })

    # çµ±è¨ˆçµæœ
    class_counts = {}
    for pred in predictions:
        class_counts[pred] = class_counts.get(pred, 0) + 1

    print(f"\nğŸ“Š é æ¸¬çµæœçµ±è¨ˆ (å…± {len(predictions)} å€‹çª—å£):")
    print("-" * 60)
    for class_id in range(5):
        count = class_counts.get(class_id, 0)
        if count > 0:
            percentage = count / len(predictions) * 100
            bar_length = int(percentage / 2)
            bar = "â–ˆ" * bar_length
            print(f"{exercise_names[class_id]:20s}: {bar:50s} {count:3d} ({percentage:5.1f}%)")

    # æœ€çµ‚é æ¸¬
    final_class = max(class_counts, key=class_counts.get)
    avg_confidence = np.mean([
        d["confidence"] for d in prediction_details if d["class"] == final_class
    ])

    print(f"\nğŸ¯ æœ€çµ‚é æ¸¬: {exercise_names[final_class]}")
    print(f"   å¹³å‡ä¿¡å¿ƒåº¦: {avg_confidence:.2%}")

    # é¡¯ç¤ºè©³ç´°æ©Ÿç‡åˆ†å¸ƒ
    print(f"\nğŸ“ˆ å„é¡åˆ¥å¹³å‡æ©Ÿç‡:")
    print("-" * 60)
    all_probs = np.array([d["probabilities"] for d in prediction_details])
    avg_probs = np.mean(all_probs, axis=0)

    for i, (name, prob) in enumerate(zip(exercise_names, avg_probs)):
        bar_length = int(prob * 50)
        bar = "â–ˆ" * bar_length + "â–‘" * (50 - bar_length)
        marker = " â† æœ€çµ‚é æ¸¬" if i == final_class else ""
        print(f"{name:20s}: {bar} {prob:.2%}{marker}")

    return {
        "final_class": final_class,
        "final_name": exercise_names[final_class],
        "confidence": avg_confidence,
        "class_counts": class_counts,
        "total_predictions": len(predictions),
        "avg_probs": avg_probs
    }

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹å¼: python test_model.py <video_path>")
        print("ç¯„ä¾‹: python test_model.py ../example_video1.mp4")
        sys.exit(1)

    video_path = sys.argv[1]

    if not Path(video_path).exists():
        print(f"âŒ æ‰¾ä¸åˆ°å½±ç‰‡æª”æ¡ˆ: {video_path}")
        sys.exit(1)

    result = test_video(video_path)

    if result:
        print("\n" + "=" * 60)
        print("âœ… æ¸¬è©¦å®Œæˆï¼")
        print("=" * 60)

        if result['confidence'] > 0.5:
            print(f"âœ… é«˜ä¿¡å¿ƒåº¦é æ¸¬: {result['final_name']} ({result['confidence']:.2%})")
        else:
            print(f"âš ï¸  ä½ä¿¡å¿ƒåº¦é æ¸¬: {result['final_name']} ({result['confidence']:.2%})")
            print("   å»ºè­°æª¢æŸ¥å½±ç‰‡å“è³ªæˆ–è¨“ç·´è³‡æ–™")
